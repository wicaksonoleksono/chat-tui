# Chat-TUI: Local Terminal Chatbot with RAG & Caching

## Overview

**Chat-TUI** is a privacy-focused, fully local terminal-based chatbot that supports:

- **TUI interface** written in Rust for fast, responsive chats.
- **Retrieval-Augmented Generation (RAG)**: Answer questions from your own documents.
- **Caching**: Persist embeddings and session data for speed and cost savings.
- **gRPC bridge**: Connect a Python Ollama server to the Rust TUI.

This guide explains architecture, setup, and contribution workflow so you can get started quickly.

---

## Architecture

```text
+----------------+         +---------------+         +--------------+
|  Rust TUI App  | <-----> |  gRPC Server  | <-----> | Ollama Model |
+----------------+         +---------------+         +--------------+
         |                         |                        |
         v                         v                        v
   Event Loop                 Retriever              LLM Inference
(keystrokes, UI)     (embed query -> search)  (chat completions)
         |                         |                        |
         v                         v                        v
  Display chat           Caching Layer           Response back
and history pane   (embeddings & sessions)
```

1. **Rust TUI**: Built on [`ratatui`](https://crates.io/crates/ratatui) + `crossterm`, handles input/output.
2. **gRPC Server**: Python app hosting Ollama model and embedding APIs via [`grpcio`](https://pypi.org/project/grpcio).
3. **Retriever**: On each query, compute embedding, query vector store, fetch top-K docs.
4. **Cache**: Store document embeddings on disk; session cache in-memory for recent queries.

---

## Features

### 1. Terminal UI

- Split pane: conversation history (scrollable) + input box.
- Command palette for actions (e.g. `:recall`, `:clear`).
- Configurable themes and keybindings.

### 2. RAG

- **Document ingestion**: Chunk PDFs/text into 500-token pieces.
- **Embedding**: Use Ollama to generate vector embeddings per chunk.
- **Vector DB**: Store embeddings in a disk-backed HNSW index (e.g. [`hnswlib-rust`](https://crates.io/crates/hnswlib)).
- **Retrieval**: For each user query, compute embedding, retrieve top-K relevant chunks, prepend them to the prompt.

### 3. Caching

- **Disk cache**: Persist document embeddings and index files under `~/.cache/chat-tui/`.
- **Session cache**: LRU in-memory cache for recent query embeddings and retrieval results.
- **Configurable TTL**: Expire session entries after N minutes to bound memory.

### 4. gRPC Integration

- Define services in `proto/chat.proto`: `Chat`, `Embed`, `Health`.
- Python server (`python/ollama_server.py`): wraps Ollama calls, exposes embedding & completion.
- Rust client: uses [`tonic`](https://docs.rs/tonic/latest/tonic/) to call Python server.

---

## Getting Started

1. **Clone**:
   ```bash
   ```

git clone [https://github.com/wicaksonoleksono/chat-tui.git](https://github.com/wicaksonoleksono/chat-tui.git)
cd chat-tui

````
2. **Install dependencies**:
- Rust: `rustup install stable`
- Python: `python3 -m venv venv && source venv/bin/activate`
- Tools:
  ```bash
pip install -r python/requirements.txt
cargo build --release
````

3. **Generate Protos** (if you change `.proto`):
   ```bash
   ```

protoc -I proto/ --python\_out=python/ --grpc\_python\_out=python/ proto/chat.proto
cargo run --bin proto\_codegen   # optional helper

````
4. **Ingest Documents** (one-time):
```bash
cargo run -- --ingest /path/to/docs/**/*.pdf
````

5. **Start gRPC Server**:
   ```bash
   ```

python python/ollama\_server.py

````
6. **Run TUI**:
```bash
cargo run -- --grpc-host 127.0.0.1 --grpc-port 50051
````

---

## Contributing

1. **Fork & Clone** your branch.  
2. **Create feature branch** (e.g. `feature/rag-cache`).  
3. **Implement** features, follow style guides:
   - Rust: `cargo fmt`, `clippy` checks.  
   - Python: `black`, `flake8`.  
4. **Test** locally (`tests/` covers RAG, caching, gRPC).  
5. **Commit & PR**: clear messages, reference issues.  
6. **Address feedback** promptly.

---

## Issue & Pull Request Labels
When opening an issue or pull request, apply one of these labels:

- **bug**: Something isn't working.
- **documentation**: Improvements or additions to documentation.
- **duplicate**: This issue or pull request already exists.
- **feature-request**: New feature or enhancement.
- **good-first-issue**: Good for newcomers.
- **help-wanted**: Extra attention is needed.
- **invalid**: This doesn't seem right.
- **question**: Further information is requested.
- **wontfix**: This will not be worked on.

---

## Project Layout

```
chat-tui/
â”œâ”€â”€ src/                 # Rust TUI & retriever
â”œâ”€â”€ python/              # gRPC server & Ollama client
â”œâ”€â”€ proto/               # .proto definitions
â”œâ”€â”€ docs/                # this documentation
â”œâ”€â”€ tests/               # unit/integration tests
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ python/requirements.txt
â””â”€â”€ README.md            # overview & quickstart
```

---

## License & Community

- **License**: GPL-3.0.
- **Discussions**: GitHub Discussions.
- **Chatroom**: Join `#chat-tui` on Matrix (coming soon).

We canâ€™t wait to see what you build with **Chat-TUI**! Feel free to open issues, PRs, or start a discussion. Happy coding! ðŸš€

